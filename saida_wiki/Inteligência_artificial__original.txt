Na informática, a Inteligência Artificial (abreviado IA) genericamente é a inteligência, o raciocínio e o aprendizado exibida por máquinas semelhante ao raciocino humano; busca desenvolver máquinas autônomas ou sistemas especialistas capazes de simular o pensamento humano e realizar várias tarefas complexas de forma independente. É o sistema que permite aos computadores executar funções avançadas, como a capacidade de analisar dados em grande escala e fazer previsões/recomendações; É um campo de pesquisa em ciência da computação que desenvolve e estuda métodos e softwares que permitem que as máquinas percebam seu ambiente e usem o aprendizado e a inteligência para tomar ações que maximizem suas chances de atingir objetivos definidos. A IA iniciou na década de 1950 com os pesquisadores Alan Turing e Herbert Simon baseado no conceito do filósofo grego Aristóteles.
Em 1950, o matemático inglês Allan Turing escreveu sobre ser possível uma máquina pensar, e imitar o comportamento humano inteligente. Também esboçou uma proposta de pesquisa para tornar isto possível.
Aplicações de IA incluem mecanismos avançados de busca na web (por exemplo, Google Search); sistemas de recomendação (usados pelo YouTube, Amazon e Netflix); assistentes virtuais (por exemplo, Google Assistant, Siri e Alexa ); veículos autônomos (por exemplo, Waymo); ferramentas generativas e criativas (por exemplo, ChatGPT, DeepSeek e AI art); e jogo e análise sobre-humanos em jogos de estratégia (por exemplo, xadrez e Go). No entanto, muitas aplicações de IA não são percebidas como IA porque já se tornaram comuns o suficiente no cotidiano das pessoas. Como por exemplo, o reconhecimento óptico de caracteres (OCR) que extrai o texto de imagens; transforma conteúdo não estruturado em dados estruturados com insights prontos para negócios; tradução de textos em idiomas estrangeiros e tradução de voz. 
A IA é um campo que abrange muitas disciplinas, como: ciência da computação, estatísticas, engenharia de hardware e de software, linguística, neurociência e, filosofia.  Vários subcampos da pesquisa em IA são centrados em objetivos específicos e no uso de ferramentas específicas. Os objetivos tradicionais da pesquisa em IA incluem raciocínio, representação de conhecimento, planejamento, aprendizagem, processamento de linguagem natural, percepção e suporte à robótica. A inteligência geral — a capacidade de completar qualquer tarefa realizada por um humano em um nível pelo menos igual — está entre os objetivos de longo prazo do campo. Para atingir esses objetivos, os pesquisadores de IA adaptaram e integraram uma ampla gama de técnicas, incluindo otimização matemática e de busca, lógica formal, redes neurais artificiais e métodos baseados em estatística, pesquisa operacional e economia. A IA também se baseia na psicologia, linguística, filosofia, neurociência e em outros campos.
A inteligência artificial foi fundada como disciplina acadêmica em 1956 e o campo passou por múltiplos ciclos de otimismo ao longo de sua história, seguidos por períodos de decepção e perda de financiamento. Os recursos e o interesse aumentaram enormemente após 2012, quando a aprendizagem profunda superou as técnicas de IA anteriores. Este crescimento acelerou ainda mais depois de 2017 e no início da década de 2020 muitos milhares de milhões de dólares estavam a ser investidos em IA e o campo experimentou um rápido progresso contínuo no que ficou conhecido como o boom da IA. O surgimento da IA generativa avançada e sua capacidade de criar e modificar conteúdo expôs diversas consequências e danos não intencionais no presente e levantou preocupações sobre os riscos da IA e seus efeitos de longo prazo no futuro, gerando discussões sobre políticas regulatórias para garantir a segurança e os benefícios da tecnologia.


== História ==

O interesse no desenvolvimento de máquinas autônomas capazes de simular o pensamento humano e de realizar varias tarefas cresceu vertiginosamente nas últimas décadas, da segunda metade do século XX, realizando assim os primeiros estudos sobre inteligência artificial (IA) a um propósito comum, a partir de iniciativas de cientistas de diversas áreas, como: psicologia, ciência cognitiva, ciência da computação e, robótica. Ferramentas eficientes em analisar problemas e oferecer soluções e planejamentos (tomada de decisão), automatização de tarefas no cotidiano das pessoas.
Mas apesar dos estudos serem modernos, o conceito de inteligência artificial não é contemporâneo; Aristóteles (professor de Alexandre, o Grande) sonhava em substituir a mão-de-obra escrava por ferramentas autônomas, sendo esta possivelmente a primeira ideia de Inteligência Artificial relatada, que a ciência da computação exploraria muito tempo depois. O desenvolvimento dessa ideia ocorreu plenamente no século XX, principalmente na década de 1950, com pensadores como Alan Turing, Herbert Simon e, John McCarthy. Turing escreveu o artigo "Computing Machinery and InteIligence" sobre a possibilidade de uma máquina pensar e imitar o comportamento humano inteligente com tal perfeição, de forma que pudesse confundir ate um juiz humano. Turing também esboçou uma proposta de pesquisa para tornar possível. Inicialmente os teste em IA foram repletos de sucessos – porém limitados devido o desempenho reduzido dos primeiros computadores - oque causava surpresa, foi o fato de um computador realizar atividade remotamente inteligente.
O sucesso inicial prosseguiu em 1957 com o General Problem Solver (GPS, Solucionador de problemas gerais) desenvolvido por Herbert Simon e Allen Newell, um programa foi projetado para imitar protocolos humanos de resolução de problemas. Dentro da classe limitada de quebra-cabeças com a qual podia lidar, verificou-se que a ordem em que os seres humanos abordavam os mesmos problemas. Desse modo, o GPS talvez tenha sido o primeiro programa a incorporar a abordagem de “pensar de forma humana”.
Em 1961, a proposta de Turing voltou no artigo de Herbert Simon e Allen Newell no artigo "The Simulation of Human Thought" sobre o teste de uma teoria de resolução humana de problemas. Esta teoria tenta explicar alguns aspectos dos processos mentais responsáveis pela inteligencia humana, um projeto de estudos  conhecido pelo nome de Projeto de Simulação Cognitiva. 
Desde o início os fundamentos da inteligência artificial tiveram o suporte de várias disciplinas que contribuíram com ideias, pontos de vista e técnicas para a IA. Os filósofos (desde 400 a.C.) tornaram a IA concebível, considerando as ideias de que a mente é, em alguns aspectos, semelhante a uma máquina, de que ela opera sobre o conhecimento codificado em alguma linguagem interna e que o pensamento pode ser usado para escolher as ações que deverão ser executadas. Por sua vez, os matemáticos forneceram as ferramentas para manipular declarações de certeza lógica, bem como declarações incertas e probabilísticas. Eles também definiram a base para a compreensão da computação e do raciocínio sobre algoritmos.
Os economistas formalizaram o problema de tomar decisões que maximizam o resultado esperado para o tomador de decisões. Os psicólogos adotaram a ideia de que os seres humanos e os animais podem ser considerados máquinas de processamento de informações. Os linguistas mostraram que o uso da linguagem se ajusta a esse modelo. Os engenheiros de computação fornecem os artefatos que tornam possíveis as aplicações de IA. Os programas de IA tendem a ser extensos e não poderiam funcionar sem os grandes avanços em velocidade e memória que a indústria de informática tem proporcionado.
Atualmente, a IA abrange uma enorme variedade de subcampos. Dentre esses subcampos está o estudo de modelos conexionistas ou redes neurais. Uma rede neural pode ser vista como um modelo matemático simplificado do funcionamento do cérebro humano. Este consiste de um número muito grande de unidades elementares de processamento, ou neurônios, que recebem e enviam estímulos elétricos uns aos outros, formando uma rede altamente interconectada.
No processamento, são compostos os estímulos recebidos conforme a intensidade de cada ligação, produzindo um único estímulo de saída. É o arranjo das interconexões entre os neurônios e as respectivas intensidades que define as principais propriedades e o funcionamento de uma RN. O estudo das redes neurais ou o conexionismo se relaciona com a capacidade dos computadores aprenderem e reconhecerem padrões. Podemos destacar também o estudo da biologia molecular na tentativa de construir vida artificial e a área da robótica, ligada à biologia e procurando construir máquinas que alojem vida artificial. Outro subcampo de estudo é a ligação da IA com a Psicologia, na tentativa de representar na máquina os mecanismos de raciocínio e de procura.
Nos últimos anos, houve uma revolução no trabalho em inteligência artificial, tanto no conteúdo quanto na metodologia. Agora, é mais comum usar as teorias existentes como bases, em vez de propor teorias inteiramente novas, fundamentar as informações em teoremas rigorosos ou na evidência experimental rígida, em vez de utilizar como base a intuição e destacar a relevância de aplicações reais em vez de exemplos hipotéticos.
A utilização da IA permite obter não somente ganhos significativos de performance, mas também possibilita o desenvolvimento de aplicações inovadoras, capazes de expandir de forma extraordinária nossos sentidos e habilidades intelectuais. Cada vez mais presente, a inteligência artificial simula o pensamento humano e se manifesta no nosso quotidiano. Em maio de 2017 no Brasil, foi criada a ABRIA (Associação Brasileira de Inteligência Artificial) com o objetivo de mapear iniciativas brasileiras no setor de  inteligência artificial, englobando os esforços entre as empresas nacionais e formação de mão de obra especializada. Esse passo reforça que, atualmente, a inteligência artificial é impactante no setor econômico.


=== Investigação na IA experimental ===
A inteligência artificial começou como um campo experimental na década de 1950 com pioneiros como Allen Newell e Herbert Simon, que fundaram o primeiro laboratório de inteligência artificial na Universidade Carnegie Mellon, e McCarty que juntamente com Marvin Minsky, que fundaram o MIT AI Lab em 1959. Foram eles alguns dos participantes na famosa conferência de verão de 1956 em Darthmouth College.
Historicamente, existem dois grandes estilos de investigação em IA: IA "neats" e IA "scruffies".  A IA "neats", limpa, clássica ou simbólica. Envolve a manipulação de símbolos e de conceitos abstractos, e é a metodologia utilizada na maior parte dos sistemas periciais.
Paralelamente a esta abordagem existe a abordagem IA "scruffies", ou  "coneccionista", da qual as redes neuronais são o melhor exemplo. Esta abordagem cria sistemas que tentam gerar inteligência pela aprendizagem e adaptação em vez da criação de sistemas desenhados com o objectivo especifico de resolver um problema. Ambas as abordagems apareceram num estágio inicial da história de IA. Nas décadas de 1960/70 os coneccionistas foram retirados do primeiro plano da investigação em IA, mas o interesse por esta vertente da IA foi retomada na década de 1980, quando as limitações da IA "limpa" começaram a ser percebidas.
Pesquisas sobre inteligência artificial foram intensamente custeadas na década de 1980 pela Agência de Projetos de Pesquisas Avançadas sobre Defesa (“Defense Advanced Research Projects Agency”), nos Estados Unidos, e pelo Projeto da Quinta Geração (“Fifth Generation Project”), no Japão. O trabalho subsidiado fracassou no sentido de produzir resultados imediatos, a despeito das promessas grandiosas de alguns praticantes de IA, o que levou proporcionalmente a grandes cortes de verbas de agências governamentais no final dos anos 80, e em consequência a um arrefecimento da atividade no setor, fase conhecida como O inverno da IA. No decorrer da década seguinte, muitos pesquisadores de IA mudaram para áreas relacionadas com metas mais modestas, tais como aprendizado de máquinas, robótica e visão computacional, muito embora pesquisas sobre IA pura continuaram em níveis reduzidos.


== Abordagens principais ==
Existem duas abordagens principais para a criação de Sistemas de Inteligência Artificial: O Simbolismo e o Conexionismo.
O Simbolismo ou IA Simbólica, propõe a representação de conhecimento por meio da manipulação de símbolos, isto é, na forma de estruturas construídas por seres humanos, normalmente baseadas em noções de Lógica. Ela teve grande impulso durante uma fase onde foram criados muitos Sistemas Especialistas, muitos deles baseados em Lógica de Primeira Ordem, implementados em Prolog, ou em linguagens de programação derivadas desta ou especializadas, como CLIPS. Normalmente programas desse tipo têm o conhecimento programado diretamente por seres humanos, o que levou a trabalhos de elicitação de conhecimento. Apesar do sucesso inicial dos Sistemas Especialistas, a grande dificuldade de levantar e registrar conhecimento a partir de humanos e o sucesso dos processos de aprendizado de máquina a partir de dados levou a dimimuição da importância dessa vertente.
O Conexionismo ou IA Conexionista, se baseia em um modelo matemático inspirado no funcionamento dos neurônios, e depende do aprendizado de máquina baseado em grandes massas de dados para calibrar esse modelo, que normalmente começa com parâmetros aleatórios. Essa abordagem, apesar de proposta muito cedo, não encontrou computadores capazes de modelar problemas complexos, apesar de ter sucesso com problemas restritos de reconhecimento de padrão, o que só acontece a partir da década de 2010, com resultados extramemente fortes no final dessa década e no início da década de 2020, a partir de modelos contendo bilhões de parametros, como o GPT-3 e conceitos como Redes Neurais Profundas, Transformers,  e Atenção.
Em torno de 2022, a maior parte da pesquisa em IA gira em torno  dos conceitos de Aprendizado de Máquina e Conexionismo, havendo também propostas para sistemas híbridos.


== Definição do termo ==
A questão sobre o que é "inteligência artificial", mesmo como definida anteriormente, pode ser separada em duas partes: "qual a natureza do artificial" e "o que é inteligência". A primeira questão é de resolução relativamente fácil, apontando no entanto para a questão de o que poderá o homem construir.
A segunda questão seria consideravelmente mais difícil, levantando a questão da consciência, identidade e mente (incluindo a mente inconsciente) juntamente com a questão de que componentes estão envolvidos no único tipo de inteligência que universalmente se aceita como estando ao alcance do nosso estudo: a inteligência do ser humano. O estudo de animais e de sistemas artificiais que não são modelos triviais começa a ser considerado como pauta de estudo na área da inteligência.
Ao conceituar inteligência artificial, presume-se a interação com o ambiente, diante de necessidades reais como relações entre indivíduos semelhantes, a disputa entre indivíduos diferentes, perseguição e fuga; além da comunicação simbólica específica de causa e efeito em diversos níveis de compreensão intuitiva, consciente ou não.
Suponhamos uma competição de cara ou coroa, cujos resultados sejam observados ou não. Se na segunda tentativa der o mesmo resultado que a primeira, então não existiam as mesmas chances para ambas opções iniciais. Claro que a coleta de informação em apenas duas amostragens é confiável apenas porque a quantidade de tentativas é divisível pelo número de opções de resultados prováveis.
A verdade é que o conceito de cara ou coroa está associado a artigos de valor, como moedas e medalhas que podem evitar que as pessoas abandonem o jogo e induza os participantes a acompanhar os resultados até o final. Para manter a disposição do adversário em desafiar a máquina seria necessário aparentar fragilidade e garantir a continuidade da partida. Isso é muito utilizado em máquinas de cassino, sendo que vários apostadores podem ser induzidos a dispensar consideráveis quantias em apostas.
A utilização de uma máquina de resultados pode compensar a ausência de um adversário, mas numa partida de xadrez, por exemplo, para que a máquina não precise armazenar todas as informações que excedem a capacidade de próprio universo imaginável são necessárias fórmulas que possam ser armazenadas para que então sejam calculadas por princípios físicos, lógicos, geométricos, e estatísticos para refletir o sistema completo em cada uma das suas partes; como a integração do Google com Wikipédia, por exemplo.
Uma popular e inicial definição de inteligência artificial, introduzida por John McCarthy na famosa conferência de Dartmouth em 1956 é "fazer a máquina comportar-se de tal forma que seja chamada inteligente caso fosse este o comportamento de um ser humano." No entanto, esta definição parece ignorar a possibilidade de existir a IA forte (ver abaixo).
Outra definição de Inteligência Artificial é a inteligência que surge de um "dispositivo artificial". A maior parte das definições podem ser categorizadas em sistemas que: "pensam como um humano; agem como um humano; pensam racionalmente ou agem racionalmente".


== Campo de estudo ==
Os principais pesquisadores e livros didáticos definem o campo como "o estudo e projeto de agentes inteligentes", onde um agente inteligente é um sistema que percebe seu ambiente e toma atitudes que maximizam suas chances de sucesso. Andreas Kaplan e Michael Haenlein definem a inteligência artificial como “uma capacidade do sistema para interpretar corretamente dados externos, aprender a partir desses dados e utilizar essas aprendizagens para atingir objetivos e tarefas específicas através de adaptação flexível”. John McCarthy, quem cunhou o termo em 1956 ("numa conferência de especialistas celebrada em Darmouth Colege" Gubern, Román: O Eros Eletrónico), a define como "a ciência e engenharia de produzir sistemas inteligentes". É uma área de pesquisa da computação dedicada a buscar métodos ou dispositivos computacionais que possuam ou multipliquem a capacidade racional do ser humano de resolver problemas, pensar ou, de forma ampla, ser inteligente. 
Nas últimas décadas, o campo exoandiu-se para incluir subáreas como aprendizado de máquina, redes neurais artificiais, processamento de linguagem natural e visão computacional, com ênfase crescente em técnicas estatísticas, métodos conexionistas e abordagens híbridas que combinam modelos simbólicos e aprendizagem profunda.. Também pode ser definida como o ramo da ciência da computação que se ocupa do comportamento inteligente ou ainda, o estudo de como fazer os computadores realizarem coisas que, atualmente, os humanos fazem melhor.


== Abordagens filosóficas ==
Não existe uma teoria ou paradigma unificador que orienta a pesquisa de IA. Pesquisadores discordam sobre várias questões. Algumas das perguntas constantes mais longas que ficaram sem resposta são as seguintes: a inteligência artificial deve simular inteligência natural, estudando psicologia ou neurociência? Ou será que a biologia humana é tão irrelevante para a pesquisa de IA como a biologia das aves é para a engenharia aeronáutica? O comportamento inteligente pode ser descrito usando princípios simples e elegantes (como lógica ou otimização)? Ou ela necessariamente requer que se resolva um grande número de problemas completamente não relacionados? A inteligência pode ser reproduzida usando símbolos de alto nível, similares às palavras e ideias? Ou ela requer processamento "sub-simbólico"? John Haugeland, que cunhou o termo GOFAI (Good Old-Fashioned Artificial Intelligence - Boa Inteligência Artificial à Moda Antiga), também propôs que a IA deve ser mais apropriadamente chamada de inteligência sintética, um termo que já foi adotado por alguns pesquisadores não-GOFAI.


=== Cibernética e simulação cerebral ===
Nos anos de 1940 e 1950, um número de pesquisadores exploraram a conexão entre neurologia, teoria da informação e cibernética. Alguns deles construíram máquinas que usaram redes eletrônicas para exibir inteligência rudimentar, como as tartarugas de W. Grey Walter e a Besta de Johns Hopkins. Muitos desses pesquisadores se reuniram para encontros da Sociedade teleológica da Universidade de Princeton e o Ratio Club na Inglaterra. Em 1960, esta abordagem foi abandonada, apesar de seus elementos serem revividos na década de 1980.


=== Sub-simbólica ===
Inteligência computacional
Interesse em redes neurais e "conexionismo" foi revivida por David Rumelhart e outros em meados de 1980. Estas e outras abordagens sub-simbólicas, como sistemas de fuzzy e computação evolucionária, são agora estudados coletivamente pela disciplina emergente inteligência computacional.


== Tipos ==


=== Tipo fraca ou inteligência artificial limitada (ANI) ===
A IA tipo fraca, limitada, ou estreita (em inglês Narrow AI) são máquinas ou sistemas inteligentes que não fazem raciocínio; é limitada porque foi projetada para fazer uma tarefa específica, após ser treinado por um humano. Ou seja, não aprende de forma autônoma. Como por exemplo reconhecer comandos de voz e encontrar a rota mais rápida.


=== Tipo forte ou inteligência artificial geral (AGI) ===
A AGI possui a habilidade de compreender e adaptar-se a vários contextos (e desafios) de forma autônoma sem a necessidade treinamento. Máquinas com inteligência versátil semelhante à humana, aprende de forma autônoma. 


=== Superinteligência artificial (ASI) ===
Considerada o futuro da IA (especulação), com a criação de máquinas com capacidade de tomar decisões e analisar dados de forma extremamente rápida, superando a IA forte.


=== Máquina reativa ou reacionária ===
Conseguem responder rapidamente as tarefas imediatas, mas não conseguem armazenar memória, não melhoraram sua funcionalidade com a experiência (não aprendem). Como por exemplo o mecanismo de recomendação da Netflix, que analisa o histórico do usuário para sugerir um filme/série.


=== Aprendizagem de máquina (Machine Learning) ===

É um subcampo da inteligência artificial, na qual uma máquina aprende, isto é, utiliza algoritmos treinados em conjuntos de dados para criar modelos de auto-aprendizagem capazes de prever resultados e classificar informação sem intervenção humana. 
A aprendizagem automática é hoje utilizada numa enorme gama de setores, seja com fins científicos (neurociências, reconhecimento de voz, novos medicamentos, equipamento), otimização de produtividade, comércio, incluindo sugestões de produtos aos consumidores por exemplo, com base em compras anteriores ou buscas automáticas online, a previsão de flutuações no mercado financeiro, ou a tradução de textos em línguas diferentes.
A IA e aprendizagem de máquinas, por seu turno; têm múltiplas subcategorias específicas, entre elas, a Aprendizagem profunda.


=== Redes neurais artificiais ===
Uma estrutura computacional inspirado na estrutura neural do ser humano para simular o cérebro humano, como por exemplo o algoritmo usado no buscador do Google; esta reconhece os termos pesquisados, busca os sinônimos e, assuntos relacionados para assim gerar melhores resultados.


=== Teoria da mente em IA ===
É um sistema inteligente com habilidade de interpretar as emoções de pessoas e animais. Um termo emprestado da psicologia, que é a habilidade dos humanos de ler as emoções dos outros e prever ações.


=== IA Autoconsciente ===
Autoconsciente ou ponto de singularidade da IA ou, como defende a academia, senciência da IA, é um estágio hipotético da inteligência artificial em que as máquinas possuem autoconsciência. Um estágio além da teoria da mente e é um dos objetivos finais no desenvolvimento da IA.
A investigação recente destaca também os impactos económicos e sociais associados a sistemas de IA avançados, incluindo a automatização de empregos, a concentração de poder tecnológico e riscos éticos ligados a uma eventual senciência. Estes fatores têm motivado debates académicos sobre regulação, governança e segurança no desenvolvimento da inteligência artificial.


=== Processamento de linguagem natural (NLP) ===
Permite que as máquinas/sistemas conversem com os humanos usando a linguagem humana, como por exemplo o uso da Alexa e da Siri. Os assistentes pessoais fazem parte da Inteligência Artificial Estreita (ANI),  limitação da IA de executar um conjunto de tarefas específicas. Os sistemas de navegação fazem tarefas bem definidas, como encontrar a rota mais rápida de um ponto a outro.


=== Visão computacional ===
Faz analise e descreve imagens e vídeos, podendo reconhecer objetos e detectar movimentos, mapear um ambiente físico.


=== Robótica autônoma ===
São máquinas ou sistemas inteligentes usados na indústria , que possuem a capacidade de tomar decisões/fazer ações programadas a partir da coleta de informações do ambiente (como por exemplo a umidade e a temperatura). Capacidade de identificar objetos e capacidade de navegar em diferentes ambientes de forma autônoma, como por exemplo a Figure AI que está construindo robôs humanoides bípedes para trabalharem com os humanos.


=== Processamento de áudio e voz ===
IA que é cria voz humana  realista, transformando texto em voz, modificação do timbre, entonação da fala e, velocidade da voz.


=== Inteligência artificial criativa ===
São sistemas inteligentes que conseguem criar roteiros, poesias, imagens e vídeos, não limitado apenas em analisar dados.


== IA forte e IA fraca ==
Entre os teóricos que estudam o que é possível fazer com a IA existe uma discussão onde se consideram duas propostas básicas: uma conhecida como "forte" e outra conhecida como "fraca". Basicamente, a hipótese da IA forte considera ser possível criar uma máquina consciente, ou seja, afirma que os sistemas artificiais devem replicar a mentalidade humana.


=== Inteligência artificial forte ===
A investigação em Inteligência Artificial Forte aborda a criação da forma de inteligência baseada em computador que consiga raciocinar e resolver problemas; uma forma de IA forte é classificada como auto-consciente. A IA forte é tema bastante controverso, pois envolve temas como consciência e fortes problemas éticos ligados ao que fazer com uma entidade que seja cognitivamente indistinguível de seres humanos. A ficção científica tratou de muitos problemas desse tipo. Isaac Asimov, por exemplo, escreveu O Homem Bicentenário, onde um robô consciente e inteligente luta para possuir um status semelhante ao de um humano na sociedade. E Steven Spielberg dirigiu "A.I. Inteligência Artificial" onde um garoto-robô procura conquistar o amor de sua "mãe", procurando uma maneira de se tornar real. Por outro lado, o mesmo Asimov reduz os robôs a servos dos seres humanos ao propor as três leis da robótica. Stephen Hawking alertou sobre os perigos da inteligência artificial e considerou uma ameaça à sobrevivência da humanidade (ver: Rebelião das máquinas).


=== Inteligência artificial fraca ===
Trata-se da noção de como lidar com problemas não determinísticos. Uma contribuição prática de Alan Turing foi o que se chamou depois de Teste de Turing (TT), de 1950: em lugar de responder à pergunta "podem-se ter computadores inteligentes?" ele formulou seu teste, que se tornou praticamente o ponto de partida da pesquisa em "Inteligência Artificial".
O teste consiste em se fazer perguntas a uma pessoa e um computador escondidos. Um computador e seus programas passam no TT se, pelas respostas, for impossível a alguém distinguir qual interlocutor é a máquina e qual é a pessoa. No seu artigo original ele fez a previsão de que até 2000 os computadores passariam seu teste. Pois bem, há um concurso anual de programas para o TT, e o resultado dos sistemas ganhadores é tão fraco (o último tem o nome "Ella") que com poucas perguntas logo percebe-se as limitações das respostas da máquina. É interessante notar que tanto a Máquina de Turing quanto o Teste de Turing talvez derivem da visão que Turing tinha de que o ser humano é uma máquina.
Há quem diga que essa visão está absolutamente errada, do ponto de vista linguístico, já que associamos à "máquina" um artefato inventado e eventualmente construído. Dizem eles: "Nenhum ser humano foi inventado ou construído". Afirma-se ainda que a comparação, feita por Turing, entre o homem e a máquina é sinônimo de sua "ingenuidade social", pois as máquinas são infinitamente mais simples do que o homem, apesar de, paradoxalmente, se afirmar que a vida é complexa. No entanto, esta linha de raciocínio é questionável, afinal de contas, os computadores modernos podem ser considerados "complexos" quando comparados ao COLOSSUS (computador cujo desenvolvimento foi liderado por Tommy Flowers, em 1943), ou a qualquer máquina do início do século XX.
A inteligência artificial fraca centra a sua investigação na criação de inteligência artificial que não é capaz de verdadeiramente raciocinar e resolver problemas. Uma tal máquina com esta característica de inteligência agiria como se fosse inteligente, mas não tem autoconsciência ou noção de si. O teste clássico para aferição da inteligência em máquinas é o Teste de Turing.
Há diversos campos dentro da IA fraca, e um deles é o Processamento de linguagem natural, que trata de estudar e tentar reproduzir os processos de desenvolvimento que resultaram no funcionamento normal da língua. Muitos destes campos utilizam softwares específicos e linguagens de programação criadas para suas finalidades. Um exemplo é o chatbot Eliza, desenvolvido por Joseph Weizenbaum no laboratório de Inteligência Artificial do MIT entre os anos de 1964 e 1966. Outro exemplo bastante conhecido é o programa A.L.I.C.E. (Artificial Linguistic Internet Computer Entity, ou Entidade Computadorizada de Linguagem Artificial para Internet), um software que simula uma conversa humana. Programado em Java e desenvolvido com regras heurísticas para os caracteres de conversação, seu desenvolvimento resultou na AIML (Artificial Intelligence Markup Language), uma linguagem específica para tais programas e seus vários clones, chamados de Alicebots.
Muito do trabalho neste campo tem sido feito com simulações em computador de inteligência baseado num conjunto predefinido de regras. Poucos têm sido os progressos na IA forte. Mas dependendo da definição de IA utilizada, pode-se dizer que avanços consideráveis na IA fraca já foram alcançados.


=== IA forte: críticas filosóficas e polêmicas ===

Muitos filósofos, sobretudo John Searle e Hubert Dreyfus, inseriram no debate questões de ordem filosófica e epistemológica, questionando qualquer possibilidade efetiva da IA forte. Seriam falsos, assim, os próprios pressupostos da construção de uma inteligência ou consciência semelhante à humana em uma máquina.
Searle é bastante conhecido por seu contra-argumento sobre o Quarto Chinês (ou Sala Chinesa), que inverte a questão colocada por Minsky a respeito do Teste de Turing. Seu argumento diz que ainda que uma máquina possa parecer falar chinês por meio de recursos de exame comparativo com mostras e tabelas de referência, binárias, isso não implica que tal máquina fale e entenda efetivamente a língua. Ou seja, demonstrar que uma máquina possa passar no Teste de Turing não necessariamente implica um ser consciente, tal como entendido em seu sentido humano. Dreyfus, em seu livro O que os computadores ainda não conseguem fazer: Uma crítica ao raciocínio artificial, argumenta que a consciência não pode ser adquirida por sistemas baseados em regras ou lógica; tampouco por sistemas que não façam parte de um corpo físico. No entanto, este último autor deixa aberta a possibilidade de um sistema robótico baseado em Redes Neuronais, ou em mecanismos semelhantes, alcançar a inteligência artificial.
Mas já não seria a referida IA forte, mas sim um correlato bem mais próximo do que se entende por IA fraca. Os revezes que a acepção primeira de Inteligência Artificial vem levando nos últimos tempos contribuíram para a imediata relativização de todo seu legado. O papel de Marvin Minsky, figura proeminente do MIT e autor de Sociedade da Mente, fora central para a acepção de uma IA linear que imitaria com perfeição a mente humana, mas seu principal feito foi construir o primeiro computador baseado em redes neurais, conhecido como Snark, tendo simplesmente fracassado pois nunca executou qualquer função interessante, apenas consumiu recursos de outras pesquisas mais promissoras. O primeiro neuro computador a obter sucesso (Mark I Perceptron) surgiu em 1957 e 1958, criado por Frank Rosenblatt, Charles Wightman e outros. Atualmente, no entanto, as vertentes que trabalham com os pressupostos da emergência e com elementos da IA fraca parecem ter ganhado proeminência do campo.
As críticas sobre a impossibilidade de criar uma inteligência em um composto artificial podem ser encontradas em Jean-François Lyotard (O Pós-humano) e Lucien Sfez (Crítica da Comunicação); uma contextualização didática do debate encontra-se em Sherry Turkle (O segundo Eu: os computadores e o espírito humano). Pode-se resumir o argumento central no fato de que a própria concepção de inteligência é humana e, nesse sentido, animal e biológica. A possibilidade de transportá-la para uma base plástica, artificial, encontra um limite claro e preciso: se uma inteligência puder ser gerada a partir destes elementos, deverá ser necessariamente diferente da humana, na medida em que o seu resultado provém da emergência de elementos totalmente diferentes dos encontrados nos humanos. A inteligência, tal como a entendemos, é essencialmente o fruto do cruzamento da uma base biológica com um complexo simbólico e cultural, impossível de ser reproduzido artificialmente.
Outros filósofos sustentam visões diferentes. Ainda que não vejam problemas com a IA fraca, entendem que há elementos suficientes para se crer na IA forte também. Daniel Dennett argumenta em Consciência Explicada que se não há uma centelha mágica ou alma nos seres humanos, então o Homem é apenas uma outra máquina. Dennett questiona por que razão o Homem-máquina deve ter uma posição privilegiada sobre todas as outras possíveis máquinas quando provido de inteligência.
Alguns autores sustentam que se a IA fraca é possível, então também o é a forte. O argumento da IA fraca, de uma inteligência imitada mas não real, desvelaria assim uma suposta validação da IA forte. Isso se daria porque, tal como entende Simon Blackburn em seu livro Think, dentre outros, não existe a possibilidade de verificar se uma inteligência é verdadeira ou não. Estes autores argumentam que toda inteligência apenas parece inteligência, sem necessariamente o ser. Parte-se do princípio que é impossível separar o que é inteligência de fato do que é apenas simulação: apenas acredita-se ser.
Estes autores rebatem os argumentos contra a IA forte dizendo que seus críticos reduzem-se a arrogantes que não podem entender a origem da vida sem uma centelha mágica, um Deus ou uma posição superior qualquer. Eles entenderiam, em última instância, máquina como algo essencialmente incapaz e sequer conseguem supô-la como capaz de inteligência. Nos termos de Minsky, a crítica contra a IA forte erra ao supor que toda inteligência derive de um sujeito - tal como indicado por Searle - e assim desconsidera a possibilidade de uma maquinaria complexa que pudesse pensar. Mas Minsky desconsidera o simples fato de que os maiores avanços na área foram conseguidos com "maquinaria complexa", também chamada por pesquisadores mais importantes de Inteligência Artificial Conexista. Se a crítica de Minsky fosse válida a maquina criada por Rosenblatt e Bernard Widrow não estaria em uso ainda hoje, e o Mark I Perceptron não seria o fundador da neuro-computação. Alguns pesquisadores importantes afirmam que um dos motivos das críticas de Minsky foi o fato de ter falhado com Snark. A partir daí começou a criticar essa área por não compreende-la completamente, prejudicando desde então pesquisas importantes sobre o assunto.
O debate sobre a IA reflete, em última instância, a própria dificuldade da ciência contemporânea em lidar efetivamente com a ausência de um primado superior. Os argumentos pró-IA forte são esclarecedores dessa questão, pois são os próprios cientistas, que durante décadas tentaram e falharam ao criar uma IA forte, que ainda procuram a existência de uma ordem superior. Ainda que a IA forte busque uma ordem dentro da própria conjugação dos elementos internos, trata-se ainda da suposição de que existe na inteligência humana uma qualidade superior que deve ser buscada, emulada e recriada. Reflete, assim, a difícil digestão do legado radical da Teoria da Evolução, onde não existe positividade alguma em ser humano e ser inteligente; trata-se apenas de um complexo de relações que propiciaram um estado particular, produto de um cruzamento temporal entre o extrato biológico e uma complexidade simbólica.
É argumentado também que a inteligência artificial ainda não é desenvolvida ao ponto de atuar como o cérebro humano, de forma criativa. Ademais, o cérebro humano ainda não é suficientemente compreendido. Portanto, a ideia de replicar funções do cérebro humano é atualmente intangível.
A IA pode ser uma arma quando usada por pessoas mal-intencionadas, como por exemplo o uso da ferramenta deepfake para golpe. A criação destes sistemas podem gerar vazamento de dados ou acidentes com veículos autônomos.


=== Impossibilidade de Simulação Qualitativa ===
Foi provado que um simulador qualitativo, completo e robusto não pode existir, ou seja, desde que o vocabulário entrada-saída seja usado (como num algoritmo QSIM), haverá sempre modelos de entrada que causam predições erradas na sua saída. Por exemplo, a noção de infinito é impossível ser tida por uma máquina finita (computador ou neurónios se produzirem apenas um número finito de resultados num número finito de tempo). Neste caso é um simples paradoxo matemático, porque são em número finito as combinações saídas de qualquer conjunto finito. Se a noção de infinito pudesse ser obtida por uma certa combinação finita, isso significaria que o infinito seria equivalente a essa sequência finita, o que é obviamente uma contradição. Por isso, o infinito e outras noções abstratas têm que ser pré-adquiridas numa máquina finita, não são aí programáveis.


== Aplicações ==

A inteligência artificial, em um contexto amplo, possui aplicações diversas, sendo empregada na resolução de problemas práticos por entidades civis, governamentais, e militares. Possui aplicações na área da saúde, mídia e comércio eletrônico, entre outros. Há uma discussão sobre como a IA tem sido integrada em sistemas de planejamento automatizado, diagnóstico médico, utilização por advogados (IA para advogados), reconhecimento de linguagem e muito mais, mostrando a ampla gama de aplicações e o impacto profundo da IA em múltiplas áreas.


=== Riscos económicos e sociais ===
A adoção ggeberalizada da inteligência artificial levanta preocupações quanto ao seu impacto no emprego e na distribuição de riqueza. Vários estudos recentes indicam que a automação de tarefas cognitivas e rotineiras pode levar ao deslocamento de trabalhadores em setores como transporte, serviços financeiros, atendimento ao cliente entre outros.  
Entre as funções com maior probabilidade de extinção, a literatura destaca tarefas como produção de conteúdo padronizado, atendimento e vendas roteirizadas, backoffice administrativo, contabilidade simples, análise repetitiva de risco, programação elementar, design baseadado em modelos, mediação transacional, formação padronizada, suporte técnico básico, revisão e formatação textual, pesquisa documental simples, edição de imagem para comércio eletrónico e até memdo o comércio electrónico na parte da fixação de preços, descrições automáticas de produtos, relatórios de gestão recorrentes, prospeção de leads genéricos, curadoria de catálogos e auditorias baseadas em listas de verificação.  
A concentração de capacidades tecnológicas em poucas empresas e países também é alvo de debate, dado o risco de aprofundar desigualdades económicas e sociais.  
Estas questões têm impulsionado propostas para políticas de requalificação profissional, regulação de algoritmos e desenvolvimento ético da IA, de modo a equilibrar eficiência tecnológica e justiça social.


== Pesquisadores ==
Atualmente existem diversos pesquisadores de IA ao redor do mundo em várias instituições e companhias de pesquisa. Entre os muitos que fizeram contribuições significativas estão:


=== Alan Turing (1912-1954) ===
O matemático britânico Alan Turing foi um dos pioneiros na área que mais tarde daria origem à Inteligência artificial. Além de ter criado o “Teste de Turing”, usado para avaliar a capacidade de uma máquina imitar respostas humanas, os seus trabalhos em lógica matemática e computação teórica estabeleceram as bases para a ciência da computação moderna.


=== John McCarthy (1927-2011) ===
Matemático, cientista, o criador do termo “inteligência artificial” e também o pai da linguagem de programação LISP. McCarthy foi considerado um dos primeiros homens a trabalhar no desenvolvimento da inteligência artificial e sempre disse que ela deveria interagir com o homem. Nascido na cidade de Boston, trabalhou na Universidade de Stanford e no Massachusetts Institute of Technology (MIT), além de ter vencido o prêmio Turing em 1972 e a Medalha Nacional de Ciência em 1991. Já a programação LISP, uma das maiores conquistas de McCarthy, surgiu em 1958 e serviu para facilitar o desenvolvimento da inteligência artificial. A linguagem é das mais antigas ainda em uso e foi usada pela primeira vez ao colocar um computador para jogar xadrez contra um adversário humano.


=== Marvin Minsky (1927-2016) ===
Natural de Nova Iorque, onde nasceu, o cientista recebeu diversos prémios internacionais pelo seu trabalho pioneiro no campo da inteligência artificial, incluindo em 1969, o Prêmio Turing, o maior prêmio em ciência informática. O cientista explorou a forma de dotar as máquinas de percepção e inteligência semelhantes à humana, criou mãos robóticas com capacidade para manipular objetos, desenvolveu novos marcos de programação e escreveu sobre assuntos filosóficos relacionados com a inteligência artificial. Minsky estava convencido de que o homem, um dia, desenvolveria máquinas que competiriam com a sua inteligência e via o cérebro como uma máquina cujo funcionamento pode ser estudado e reproduzido num computador, o que poderia ajudar a compreender melhor o cérebro humano e as funções mentais superiores.


=== Raj Reddy (1937) ===
Informático indiano naturalizado estadunidense, foi o primeiro asiático a vencer o Prêmio Turing. Entre suas contribuições para a IA estão a criação do Instituto de Robótica da CMU e demonstrações de diversos sistemas que usam alguma forma de IA. Entre esses sistemas, estão sistemas de: fala, controlados por voz, reconhecimento de voz, reconhecimento de voz independente do interlocutor, etc. Para Reddy, ao invés de substituir a humanidade, a tecnologia irá criar um novo tipo de humano que irá coexistir com seus antecessores enquanto se aproveita das vantagens de uma nova classe de ferramentas viabilizada pela tecnologia.


=== Terry Winograd (1946) ===
Winograd é um cientista da computação estadunidense, professor da Universidade Stanford, e codiretor do grupo de interação humano-computador de Stanford. É conhecido nas áreas de filosofia da mente e inteligência artificial por seu trabalho sobre língua natural usando o programa SHRDLU. Para Terry, não restam dúvidas de que a tecnologia da informática, mais precisamente a área de inteligência artificial, transformará as sociedades, introduzindo modificações socioeconômicas irreversíveis. Esse especialista procura saber se os seres humanos seriam capazes de construir máquinas que poderiam compreende-los, resolver seus problemas e dirigir suas vidas, além de buscar respostas sobre o que aconteceria se, algum dia, essas máquinas se tornassem mais inteligentes do que os próprios humanos que as criaram.


=== Douglas Lenat (1950) ===
Nascido na Filadélfia, Pensilvânia, se formou na Universidade da Pensilvânia. Douglas Bruce Lenat é o Diretor Executivo do Cycorp e foi também um pesquisador proeminente em inteligência artificial, recebendo o prêmio bianual IJCAI Computers and Thought em 1976 pela criação do programa de aprendizado de máquinas. Ele também trabalhou em  simulações militares e em numerosos projetos para organizações governamentais, militares, científicas e de inteligência dos EUA. A missão de Lenat, no longo ciclo do projeto Cyc, iniciado em 1984, era de construir a base de uma inteligência artificial geral ao representar manualmente o conhecimento como axiomas lógicos contextualizados na linguagem formal com base em extensões ao cálculo de predicados de primeira ordem e em seguida, usar esse enorme motor de inferência de ontologia e a base de conhecimento contextualizada como um viés indutivo para automatizar e acelerar cada vez mais a educação contínua do próprio Cyc, via aprendizagem em máquina e compreensão da linguagem natural.


== Ver também ==
Ablação (inteligência artificial)
Alinhamento da inteligência artificial
Lista de projetos de inteligência artificial
Segurança da inteligência artificial
OpenAI
OpenAI Codex
Realidade simulada
Robocode
Transferência de energia sem fio


== Referências ==


=== Bibliografia ===


== Ligações externas ==
«Programa do governo para defender suas questões». (em português) 
«American Association for Artificial Intelligence» (em inglês) 
História da IA no YouTube