Processamento de linguagem natural (PLN) é um campo da ciência da computação, inteligência artificial e linguística que lida com a compreensão e geração automática de linguagens humanas naturais. Sistemas de geração de linguagem natural convertem informações de bancos de dados em linguagem compreensível para humanos, enquanto sistemas de compreensão de linguagem natural transformam linguagem humana em representações formais para manipulação computacional. Desafios incluem a extração de significado da linguagem humana (compreensão de linguagem natural) e a produção de linguagem humana (geração de linguagem natural).

O campo surgiu na década de 1950, influenciado pelo teste de Turing proposto por Alan Turing. A experiência de Georgetown em 1954 demonstrou a tradução automática do russo para o inglês, embora as previsões de resolução rápida do problema não se concretizaram. Após um relatório crítico em 1966, o financiamento para a tradução automática diminuiu, ressurgindo no final dos anos 1980 com sistemas estatísticos. Sistemas iniciais notáveis incluem SHRDLU e ELIZA. A década de 1970 viu o desenvolvimento de "ontologias conceituais" e chatterbots como PARRY e Racter.

Inicialmente, sistemas de PLN dependiam de regras complexas elaboradas manualmente. No final dos anos 1980, algoritmos de aprendizado de máquina revolucionaram o campo, impulsionados pelo aumento do poder computacional e pela mudança de foco das teorias linguísticas chomskyanas. Algoritmos de aprendizado de máquina mais antigos produziam sistemas de regras rígidas, enquanto a marcação de partes do discurso introduziu modelos ocultos de Markov. A pesquisa subsequentemente se concentrou em modelos estatísticos probabilísticos.

A tradução automática foi uma área inicial de sucesso, impulsionada pelo trabalho da IBM e pela disponibilidade de corpora textuais multilíngues produzidos pelo Parlamento do Canadá e pela União Europeia. A pesquisa recente tem explorado algoritmos de aprendizado semi-supervisionados e não supervisionados para aproveitar grandes quantidades de dados não anotados.

Os algoritmos modernos de PLN utilizam aprendizado de máquina, particularmente aprendizado de máquina estatístico. Ao contrário da codificação direta de regras, o aprendizado de máquina induz regras a partir de corpora de exemplos reais. Esses algoritmos utilizam um conjunto de "recursos" extraídos dos dados de entrada. Modelos estatísticos, que atribuem pesos a características de entrada, tornaram-se predominantes.

Tarefas comuns em PLN incluem sumarização automática, resolução de correferência, análise do discurso e tradução automática. Outras tarefas incluem segmentação morfológica. Estas tarefas são caracterizadas por definições de problemas bem especificadas, métricas de avaliação padrão e corpora para avaliação.