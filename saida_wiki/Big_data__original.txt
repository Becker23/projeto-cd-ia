Big data (macrodados, megadados, ou grandes dados em português) é a área do conhecimento que estuda como tratar, analisar e obter informações a partir de conjuntos de dados muito grandes. O termo big data surgiu em 1997, e foi inicialmente utilizado para nomear conjuntos de dados não ordenados em rápido crescimento. Nas últimas décadas, os conjuntos de dados têm crescido de forma exponencial.[carece de fontes?] 
Com o aparecimento da Internet, a quantidade de dados disponíveis aumentou abruptamente: da "era" do terabyte para o petabyte, e a desde 2015, a era do zettabyte. Atualmente são gerados mais de 2,5 quintilhões de bytes diariamente. Pela sua presença nas relações econômicas e sociais, representou uma evolução nos sistemas de mercado e na ciência. As ferramentas que fazem uso de big data são de grande importância, por exemplo, para definir estratégias de marketing, aumentar a produtividade, reduzir custos e tomar melhores decisões. 
Um dos pontos essenciais do conceito de big data é o fato de ter sido capaz de gerar valor para empresas e para o mercado. No que diz respeito à ciência, o surgimento de big data levou à criação de um novo paradigma (4° paradigma), concebendo um novo método para ampliar as fronteiras do conhecimento. Graças às novas tecnologias, é possível recolher, manipular, analisar e exibir dados com mais eficácia, aumentando o valor agregado das análises geradas.


== Definição ==
Big data é um termo recente e, por isso, não existe na maior parte dos dicionários de estatística. São dados multivariados e de elevada dimensão, geralmente criados em tempo real, e apresentam um crescimento exponencial (na escala temporal), nomeados de megadados.
Quanto mais dados são gerados, maior é o esforço para extrair informações. Os centros de dados tiveram que aprender a lidar com o crescimento exponencial de dados gerados e desenvolver ferramentas que fossem para além de bancos de dados relacionais e sistemas paralelos de bancos de dados. Sendo assim, a velocidade para obter a informação faz parte do sucesso que o big data pode proporcionar em sua empresa. O conceito de big data foi definido inicialmente por 3'V, mas a literatura mostrou que seu conceito pode ser expandido para 5'V, representados pelos seguintes conceitos:

Volume: relacionado à grande quantidade de dados gerados, devido a extensa variedade de fontes de dados, onde as empresas coletam estes. Anos atrás, armazenar essa grande quantidade de dados e informações era um impasse, principalmente quando se tratava de processamento, armazenamento e organização, devido ao alto custo dos equipamentos e à pouca oferta. Até o ano 2000, aproximadamente 25% dos dados eram digitais. Muitas das informações/dados produzidos estavam em papéis, livros e outros tipos de armazenamento de informações físicas. Já nos anos de 2012 e 2014, este percentual passou para 98%. Isso se deve à queda dos preços dos computadores e celulares, que se tornaram mais acessíveis. Analisando a conjuntura social e tecnológica atual, percebe-se que cada pessoa em suas atividades cotidianas e rotineiras produz uma infinidade de dados e informações, desde preferências musicais, localização, meio de transportes, locais que visitou etc. Estima-se que a cada 18 meses, o volume de informações produzidas dobra.
Variedade: as fontes de dados são muito variadas, o que aumenta a complexidade das análises. Essas fontes podem ser desde transações comerciais, redes sociais, radares, sensores, celulares, computadores, até sites de reclamações, “curtidas” nas redes sociais, coordenadas de GPS, drones, etc. Além das variedades das fontes, existem também os diferentes formatos e tipos de dados, desde dados estruturados, dados numéricos em bancos de dados tradicionais, até os dados não estruturados, como textos, e-mail, vídeos, áudios e dados de transações financeiras. Um dos maiores desafios do big data é gerenciar todos esses diferentes tipos de dados e fontes ao mesmo tempo.
Velocidade: Devido ao grande volume e variedade de dados, todo o processamento deve ser ágil para gerar as informações necessárias. Os dados são cada vez mais demandados em tempo real, com o menor delay (tempo de espera) possível, devendo ser tratados em tempo hábil. Em 2012, aproximadamente 2,5 exabytes de dados e informações foram produzidos no planeta pela humanidade. Isso corresponde a aproximadamente 29 terabytes de dados por segundo. A tendência é que essa produção aumente com a disseminação da IoT (Internet das Coisas).
Veracidade: A veracidade está ligada diretamente ao quanto uma informação é verídica. É a necessidade de garantir que os dados coletados sejam autênticos. Isso se aplica às fontes de dados que precisam ser confiáveis e aos dados e informações propriamente ditos. É importante lembrar que os sistemas também podem produzir dados com erros.
Valor: Este conceito está relacionado com o valor obtido desses dados, ou seja, com a “informação útil”, sendo o ponto mais destacado em relação às aplicações do big data.


== Histórico ==
Construção do Conceito
O termo big data tem um conceito relativo, já que seu tamanho depende de quem está usando os dados. Neste contexto, o primeiro relato sobre uso de estatísticas para obter informações de grandes quantidades de dados data de 1663. Nesse ano, John Graunt utilizou uma grande quantidade de informações, de diferentes fontes, para estudar a epidemia da peste bubônica na Europa. Para Graunt, sua quantidade de dados poderia ser considerada big data
O uso dos primeiros equipamentos para processar dados são de 1890, durante a realização do Censo dos Estados Unidos, conduzido pelo U.S. Census Bureau. Na ocasião, a Máquina de Tabulação diminuiu o tempo de processamento dos dados para apenas seis semanas. Entretanto, somente no século XX que começaram a surgir os primeiros sistemas para armazenamento de informações. Em 1927, o engenheiro Fritz Pfleumer criou um método para guardar informações em fitas magnéticas.
Durante a Segunda Guerra Mundial, foi criada a primeira máquina digital de processamento de dados. Foi em 1943, quando os britânicos desenvolveram um sistema para decifrar códigos nazistas durante a Segunda Guerra Mundial. O nome da máquina era Colossus, que podia interceptar mensagens a uma taxa de 5000 caracteres por segundo. O primeiro órgão público criado especificamente para o processamento de dados, a Agência Nacional de Segurança (NSA) dos EUA, foi fundado em 1952, com o objetivo de processar dados automaticamente para obter informações relativas a inteligência durante a Guerra Fria.
Um dos primeiros Centros de Dados foi criado em 1965, também pelo governo americano, com o objetivo de controlar o pagamento de impostos e as impressões digitais dos americanos. Esse Centro de Dados possuía o mesmo padrão dos bancos de dados criados até a década de 1970. Eram bancos de dados centralizados, onde uma mesma máquina era responsável pelo uso, armazenamento e análise dos dados. Com o aumento da quantidade de dados, começaram a surgir novas arquiteturas de dados que permitissem processar e analisar esses dados. Na década de 80, começaram a surgir os Sistemas de Bancos de Dados Paralelos. Nesse caso, ao invés de um banco de dados centralizado, cada processador se comunica com os outros apenas enviando mensagens através de uma rede interconectada. Os primeiros bancos de dados paralelos possibilitaram a criação do primeiro banco de dados com capacidade em terabytes, pela KMART, em 1986.
Em 1989, o cientista britânico Tim Berners-Lee criou o World Wide Web, para facilitar a troca de informações entre as pessoas. O que Tim Berners-Lee não sabia era que sua invenção iria revolucionar a forma como os dados eram gerados e a quantidade de dados criados. A criação da Web 2.0 ajudou no aumento dos dados. O termo big data foi usado pela primeira vez em 1997. Entretanto, o nome começou a ser usado oficialmente em 2005, quando Roger Mougalas, da O’Reilly Media, publicou um artigo mencionando o tema.
Evolução Tecnológica de Armazenamento e Processamento
Os dados que agregam o conjunto do big data são provenientes de várias fontes. Desta maneira, normalmente não apresenta uma estrutura bem definida, ou seja, não pode ser armazenada nos sistemas padrões de banco de dados, como o Sistema Gerenciador de Banco de Dados Relacional (SGBDR), onde os dados são representados por meio de tabelas, com diversas linhas e colunas. Os cientistas de dados começaram a verificar que bancos de dados relacionais não conseguiriam suportar essa grande quantidade de dados não estruturados. Desta maneira, novas tecnologias e processos tiveram que ser desenvolvidos para permitir que esses dados não estruturados fossem analisados, já que os mesmos podem representar até 80% do total de dados. Foi quando a Google criou o MapReduce, em 2004, que é um modelo de programação que permite processar grandes quantidades de dados em paralelo, dividindo o trabalho em um conjunto de tarefas independentes, geralmente executado em um cluster de computadores.
Posteriormente, foi desenvolvido o Hadoop, que é uma implementação em código aberto do MapReduce. O Hadoop foi criado pelo Yahoo em 2005, e pode ser considerado uma das maiores invenções de data management desde o modelo relacional. Entretanto, o Hadoop não é considerado uma base de dados como o SGBDR. Ele é um sistema de distribuição de arquivos utilizado para processar e armazenar uma grande quantidade de dados (big data) por meio de clusters, onde os mesmos são processados paralelamente; e podendo ser executados em servidores sem muito esforço. Atualmente, esse tipo de processamento é o mais utilizado por empresas que trabalham com big data; e diversas empresas vêm contribuindo com código para seu desenvolvimento, como a Yahoo, Facebook, Cloudera, IBM e outras.
Segundo a IBM, em 2008 foram produzidos cerca de 2,5 quintilhões de bytes todos os dias; e surpreendentemente 90% dos dados no mundo foram criados nos últimos dois anos, decorrente à adesão das grandes empresas à internet, como por exemplo as redes sociais, dados dos GPS, dispositivos embutidos e móveis, dentre outros. Atualmente, a Internet das Coisas mudou a forma como os dados são gerados, aumentando de forma abrupta a quantidade destes. Todos esses objetos físicos da Internet das Coisas são capazes de coletar e transmitir dados, gerando dados não estruturados que não podem ser armazenados e processados por bancos de dados comuns.


== Mercado de trabalho ==
As oportunidades de trabalho na área de estatística estão aumentando graças à proliferação de programas para análise de dados e seu uso, especialmente, na tomada de decisão com objetivos estratégicos como: políticas de governo, seleção de investimentos, gestão de empresas e negócios, etc. O big data permite trabalhar com grandes volumes de dados, por vezes, não aceites pelos grandes programas estatísticos. No Brasil existe a profissão de Estatístico, regulamentada pelo Decreto Federal nº 62497 de 1968. Este profissional é treinado para trabalhar com estruturas de dados, em seu manuseio para extração de informação estratégica, nos métodos estatísticos de análise e em programação para sua análise estatística, de modo a se obter conclusões com margens de erro controladas para a tomada de decisões com base nos dados disponíveis. A IBM criou a Big Data University, que fornece certo conhecimento do big data. Existem na Internet, sites que oferecem plataformas de ensino à distância, comummente conhecidas como MOOCs, com cursos nas áreas de big data e de ciência de dados (Data Science, no original em inglês), nos quais pode-se estudar o seu conteúdo de forma gratuita ou pagar pelo certificado do curso. 
Os mais conhecidos são os sites do Coursera, Udacity e o EDX.org, este último, fruto de parceria entre as universidades americanas de Harvard e do MIT e empresas do Vale do Silício. No Brasil, o mercado para a área é promissor, sendo que muitas renomadas Universidades passaram a oferecer cursos de pós-graduação e MBAs ligados à área de big data, variando em sua maioria no tamanho da carga horária destinada à parte de negócios, componente importante na formação deste profissional, que precisará ter além das habilidades técnicas, a capacidade de apresentar as conclusões de suas análises e insights para um público leigo de forma simples, de forma a gerar valor para o negócio da empresa.
Abaixo, é possível apontar quatro áreas de atuação do cientista de dados e os domínios necessários na era do big data:

a) Tecnologia: Além das habilidades de manejo e familiaridade no aparato tecnológico, o profissional terá maior vantagem ao gerenciar dados se tiver domínios em computação em nuvem, linguagem de programação, segurança e privacidade dos dados, infraestrutura conceitos de Map Reduce, manejo de programas e software;
b) Análise de dados: O profissional necessita ter domínio de método científico, Data Science, análise de gráficos, mineração de dados e estatística;
c) Gestão de dados: O profissional deve ter domínios em Big Data (Hadoop, SQL, Spark), gestão de dados estruturados (RDB, XML), gestão de dados não-estruturados (bancos de dados NoSQL), recuperação dos dados e inteligência competitiva;
d) Design e Comunicação: Nesta área o profissional deve ter domínios em comunicação interpessoal, visualização de dados, habilidades de contar histórias visuais, proatividade, poder de negociação e flexibilidade;
e) Matemática e estatística: O profissional deve ter domínios de estatística avançada,  gestão de manipulação de grandes volumes de dados, relacionar, descobrir padrões e cruzar dados estatísticos, data mining e otimização.


== Aplicações na atualidade ==
Um estudo do Instituto IDC mostrou que diversos setores da sociedade estão investindo em big data, indicando que foram investidos mais de US$ 16,6 bilhões em 2014 para atividades do setor. Este mesmo estudo afirma que a expectativa é que este valor atinja, em 2018, o valor de US$ 41,5 bilhões. As instituições estão investindo em big data por observarem da interferência dos custos, das consequências que pode haver para o futuro do negócio. O objetivo por trás do big data é melhorar a prestação de informações aos gestores, fazendo com que haja um suporte na tomada de decisões – com dados reais e precisos. A seguir serão apresentadas algumas aplicações de big data, em diferentes setores:

O filme “Moneyball” (O homem que mudou o jogo) com o ator Brad Pitt, no qual o gerente de um time de beisebol usa o big data para reunir um time de primeira linha sem gastar muito;
A empresa UPS, após análise das rotas de seus motoristas, proibiu os mesmos de virar à esquerda. De acordo com a empresa, isto permitiu economizar por ano cerca de 38 milhões de litros de combustível, deixando de emitir 20 mil toneladas de dióxido de carbono. Além disso, entregam 350 mil pacotes a mais;
No terremoto do Haiti, pesquisadores americanos fizeram uso da geolocalização de 2 milhões de chips SIM, para auxiliar nas missões humanitárias;
Para melhorar os laboratórios de física nuclear, a empresa CERN (Organização Europeia para a Pesquisa Nuclear), criou o maior acelerador de partículas do mundo, chamado Large Hadron Collider. Com ele, é gerada uma quantidade enorme de dados. Para a utilização dessa máquina é necessário muita memória - cerca de 30 petabytes de dados - e, para analisar esses dados são necessários 65 mil processadores, e usa também o recurso de vários computadores pelo mundo inteiro.
A utilização dos dados de censos e outros recolhidos pelos governos, facilita na análise dos Datas Censes, melhorando a nossa saúde e também ciência social.
Em busca dos melhores lugares para instalar turbinas eólicas, uma empresa dinamarquesa analisou petabytes de dados climáticos do nível das marés, mapas de desmatamentos, entre outros. No fim o que costumava demorar semanas durou apenas algumas horas;
Big data foi de grande importância para o descobrimento do pré-sal, devido a sua velocidade, que agilizava os processamentos de dados sísmicos captados pelas sondas que procuram petróleo no fundo do mar. Como são milhões as variáveis, o trabalho exige intermináveis simulações de imagens, e só o big data é capaz de dar conta do trabalho em um tempo melhor;
Alguns times de diversos esportes utilizam o big data na performance dos atletas, com câmeras e outros aparelhos. Desta maneira, conseguem observar o desempenho dos atletas e, ao analisar os dados, tomar decisões mais precisas, melhorando o desempenho e corrigindo os erros, criando também estatísticas para os próximos jogos;
Empresas de tecnologia como a Netflix e a Spotify utilizam de big data para definir as preferências dos seus usuários, e fornecer para eles conteúdos mais individualizados;
As ferramentas de propaganda do Facebook e do Instagram são baseadas em big data, pois correlacionam dados dos usuários das redes sociais com suas preferências de consumos e serviços.
Um interessante estudo de caso sobre sucessos e erros do uso do big data é o Google Flu Trends (GTF), que foi lançado pela Google em 2008.. Este serviço foi divulgado pela primeira vez por meio de um artigo na revista Nature, e prometia detectar com algumas semanas de antecedência a ocorrência de epidemias de gripe. Anteriormente ao GFT, os sistemas tradicionais dos EUA faziam estimativas de casos e epidemias de gripe a cada duas semanas, usando dados dos Centros de Controle e Prevenção de Doenças dos EUA (U.S. Centers for Disease Control and Prevention - CDC). Essas estimativas eram baseadas em dados virológicos e clínicos, relacionados a visitas de pacientes aos hospitais e consultórios. Com o aumento do acesso à internet, verificou-se que mais de 90 milhões de americanos procuravam todos os anos por informações sobre uma doença específica ou problema médico. Nesse contexto, os pesquisadores da Google verificaram que era possível correlacionar essas buscas com casos efetivos de gripe. Para validar a metodologia, foram processadas centenas de bilhões de pesquisas no Google pelo período de 5 anos (2003 a 2007) para os EUA, correlacionando palavras de busca específicas com casos efetivos da doença. Os dados foram validados por meio dos relatórios da CDC para o período, com um correlação média de 90%. O modelo foi testado em tempo real nos anos de 2007 e 2008 e os resultados foram divulgados com o CDC para avaliar a resposta e a acurácia, mostraram a possibilidade de prever casos de gripe em uma a duas semanas antes do CDC. A partir desses resultados, o serviço começou a ser utilizado operacionalmente para outros países, realizando estimativas de epidemias de gripe para mais de 25 nações. Entretanto, atualmente o serviço não está mais ativo, mas estimativas históricas ainda estão disponíveis para download. Isso aconteceu pelos erros subsequentes nas previsões realizadas pelo serviço nos anos posteriores. Isso aconteceu em 2013, quando o sistema não previu uma epidemia de gripe, ou como a epidemia da gripe H1N1, em 2009 Em um artigo publicado na revista Science, pesquisadores indicaram os seguintes fatores como os causadores dos problemas e das falhas com o serviço GFT:


== Críticas ==
A massificação de dados, no entanto, ainda enfrenta obstáculos. O maior deles seria a privacidade, ou seja, a ameaça à privacidade representada pelo aumento de armazenamento e integração de informações pessoalmente identificáveis. Se a recomendação de links patrocinados pelo Google já parece invasiva à maioria das pessoas, o mundo e a legislação atual não estão preparadas para as possibilidades que o big data oferece de agregar, analisar e tirar conclusões a partir de dados até então esparsos. Painéis de especialistas lançaram várias recomendações de políticas para adequar a prática às expectativas de privacidade.
O big data já foi relacionado como ferramenta essencial em manipulação de eleições e disseminação de fake news, isso se dá pela capacidade inerente da tecnologia de reunir e segmentar um determinado público alvo, fazendo com que campanhas de marketing sejam muito mais efetivas e impactantes, isso faz do big data uma metodologia questionável do ponto de vista ético, uma vez que pode ser usado para manipular massas e obter resultados parciais de acordo com a motivação dos especialistas.
Big data tem sido usado no policiamento e vigilância por instituições como as autoridades policiais e corporações. Devido à natureza menos visível da vigilância baseada em dados em comparação aos métodos tradicionais de policiamento, é menos provável que surjam objeções ao policiamento de big data. De acordo com o Big Data Surveillance: The Case of Policing, de Sarah Brayne, o policiamento de big data pode reproduzir as desigualdades sociais existentes de três maneiras:

Colocar suspeitos de crimes sob vigilância aumentada usando a justificativa de um algoritmo matemático e, portanto, imparcial
Aumentar o escopo e o número de pessoas sujeitas ao rastreamento da aplicação da lei e exacerbar a representação racial existente no sistema de justiça criminal
Estimular os membros da sociedade a abandonar as interações com instituições que criariam um traço digital, criando obstáculos à inclusão social.


== Ver também ==
Ciência de dados
Visualização de dados
Dataísmo
Internet das coisas


== Referências ==


== Bibliografia ==
Hilbert, Martin; Priscila Lopez (2011). «The World's Technological Capacity to Store, Communicate, and Compute Information». Science. 332 (6025): 60–65. PMID 21310967. doi:10.1126/science.1200970  
Grobelnik, Marko (2012). «Big Data Tutorial». videolectures.net. 2nd ESWC Summer School 2012 


== Ligações externas ==
Planejamento de Vendas: O que é Big Data?.
Profissionais TI: O que é Big Data e Como Funciona?