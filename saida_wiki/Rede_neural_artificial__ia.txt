Redes neurais artificiais (RNAs), também conhecidas como redes neuronais artificiais, são modelos computacionais inspirados no sistema nervoso central animal, particularmente no cérebro. Esses modelos são utilizados em aprendizado de máquina e reconhecimento de padrões, e são geralmente apresentados como sistemas de neurônios interconectados capazes de computar valores a partir de entradas, simulando o comportamento de redes neurais biológicas.

Uma RNA para reconhecimento de escrita manual, por exemplo, pode ser definida por um conjunto de neurônios de entrada ativados pelos pixels de uma imagem. Os dados dessa ativação são ponderados, transformados por uma função e repassados a outros neurônios, um processo repetido até a ativação de um neurônio de saída, determinando o caractere lido.

RNAs são usadas para resolver tarefas difíceis para a programação tradicional baseada em regras, incluindo visão computacional e reconhecimento de voz. O aprendizado em RNAs ocorre por meio de ajustes iterativos em seus "pesos", permitindo que generalizem soluções para classes de problemas. Algoritmos de aprendizagem definem as regras para a solução de problemas de aprendizado, e os paradigmas incluem aprendizado supervisionado, não supervisionado (auto-organização) e por reforço.

Aplicações de RNAs abrangem áreas como reconhecimento automático de alvos, reconhecimento de caracteres, robótica, diagnóstico médico, sensoriamento remoto, processamento de voz, biometria, análise de dados (data mining), incluindo análise de imagem de sistemas quânticos, análise de disfluência da fala, tratamento corretivo do aparelho buco-maxilar, predição da Doença de Alzheimer, reconhecimento de câncer, análise do planejamento urbano, tomada de decisão em engenharia de produção e mercado financeiro.

As primeiras informações sobre neurocomputação surgiram em 1943, com trabalhos de Warren McCulloch e Walter Pitts, que propuseram um modelo de "neurônios formais" simulando o comportamento de neurônios naturais. Outras influências iniciais incluem a obra de von Neumann sobre a teoria de autômatos, a máquina de Turing e o livro "Cybernetics" de Wiener. Hebb introduziu a Regra de Aprendizagem de Hebb em 1949, que descreve um sistema de aprendizado por correlação de neurônios.

Em 1951, foi construído o primeiro neurocomputador, Snark, por Mavin Minsky. Rosenblatt criou o Perceptron em 1958, um modelo cognitivo capaz de aprender tudo o que pudesse representar. No início da década de 1960, Widrow e Hoff publicaram um artigo sobre o ADALINE, um neurônio artificial com a regra de aprendizagem Delta. A publicação de "Perceptrons" de Minsky e Papert em 1969 expôs limitações do modelo de Rosenblatt.