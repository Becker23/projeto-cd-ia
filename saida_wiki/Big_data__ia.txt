*Big data* refere-se à área que explora o processamento, análise e interpretação de grandes conjuntos de dados. O termo surgiu em 1997, inicialmente associado a conjuntos de dados não ordenados e de rápido crescimento. O volume de dados disponíveis aumentou exponencialmente com o advento da Internet, evoluindo do terabyte ao petabyte e, a partir de 2015, ao zettabyte, com a geração diária de mais de 2,5 quintilhões de bytes. Esse fenômeno impactou as relações econômicas e sociais, influenciando tanto os sistemas de mercado quanto a pesquisa científica. Ferramentas baseadas em *big data* são utilizadas para definir estratégias de marketing, aumentar a produtividade, reduzir custos e otimizar a tomada de decisões.

Um aspecto central do conceito é a capacidade de gerar valor para empresas e para o mercado. No campo científico, o surgimento do *big data* impulsionou um novo paradigma, ampliando as fronteiras do conhecimento. O conceito frequentemente é caracterizado pelos "5 Vs": Volume (grande quantidade de dados), Variedade (diversidade de fontes e formatos), Velocidade (rapidez no processamento), Veracidade (autenticidade dos dados) e Valor (informação útil extraída).

O uso de estatísticas para analisar grandes volumes de dados remonta a 1663, quando John Graunt estudou a epidemia de peste bubônica na Europa. Em 1890, a Máquina de Tabulação foi utilizada no Censo dos Estados Unidos para reduzir o tempo de processamento dos dados. No século XX, surgiram os primeiros sistemas de armazenamento de informações, como fitas magnéticas em 1927. Durante a Segunda Guerra Mundial, a máquina Colossus foi desenvolvida para decifrar códigos nazistas. Em 1952, a Agência Nacional de Segurança (NSA) dos EUA foi criada para processar dados automaticamente para fins de inteligência. Um dos primeiros centros de dados foi estabelecido em 1965 pelo governo americano para controlar o pagamento de impostos e impressões digitais. Na década de 1980, surgiram os Sistemas de Bancos de Dados Paralelos, possibilitando a criação do primeiro banco de dados com capacidade em terabytes pela KMART em 1986.

Em 1989, Tim Berners-Lee criou o World Wide Web, que revolucionou a forma como os dados são gerados. O termo *big data* foi usado pela primeira vez em 1997, e ganhou reconhecimento em 2005 com a publicação de um artigo de Roger Mougalas.

A análise de *big data* frequentemente envolve dados não estruturados, que não podem ser armazenados em sistemas de banco de dados relacionais tradicionais. Em 2004, a Google criou o MapReduce, um modelo de programação para processar grandes quantidades de dados em paralelo. O Hadoop, uma implementação em código aberto do MapReduce, foi criado pelo Yahoo em 2005. A Internet das Coisas (IoT) aumentou a quantidade de dados gerados, com objetos físicos capazes de coletar e transmitir dados não estruturados.